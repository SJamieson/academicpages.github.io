
@inproceedings{Girdhar2019_ICRA,
  ids = {Girdhar2019},
  title = {Streaming {{Scene Maps}} for {{Co}}-{{Robotic Exploration}} in {{Bandwidth Limited Environments}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Girdhar, Yogesh and Cai, Levi and Jamieson, Stewart and McGuire, Nathan and Flaspohler, Genevieve and Suman, Stefano and Claus, Brian},
  year = {2019},
  month = may,
  pages = {7940--7946},
  publisher = {{IEEE}},
  address = {{Montr\'eal, Canada}},
  issn = {2577-087X},
  doi = {10/ggb46q},
  url = {https://arxiv.org/abs/1903.03214},
  abstract = {This paper proposes a bandwidth tunable technique for real-time probabilistic scene modeling and mapping to enable co-robotic exploration in communication constrained environments such as the deep sea. The parameters of the system enable the user to characterize the scene complexity represented by the map, which in turn determines the bandwidth requirements. The approach is demonstrated using an underwater robot that learns an unsupervised scene model of the environment and then uses this scene model to communicate the spatial distribution of various high-level semantic scene constructs to a human operator. Preliminary experiments in an artificially constructed tank environment as well as simulated missions over a 10m\texttimes 10m coral reef using real data show the tunability of the maps to different bandwidth constraints and science interests. To our knowledge this is the first paper to quantity how the free parameters of the unsupervised scene model impact both the scientific utility of and bandwidth required to communicate the resulting scene model.},
  copyright = {All rights reserved},
  file = {/home/stewart/sync/Literature/IEEE/Girdhar et al. - 2019 - Streaming Scene Maps for Co-Robotic Exploration in Bandwidth Limited Environments.pdf;/home/stewart/sync/Zotero/storage/I8LHW63H/Girdhar et al. - 2019 - Streaming Scene Maps for Co-Robotic Exploration in.pdf;/home/stewart/sync/Zotero/storage/USQ3NW5W/8794132.html},
  isbn = {978-1-5386-6027-0},
  keywords = {Bandwidth,Bayes methods,Data models,Oceans,Robot sensing systems,Visualization}
}

@phdthesis{Jamieson2018,
  title = {Deep {{Learning}} for {{Robust Vision}} in {{Realtime Autonomous Driving}}},
  author = {Jamieson, Stewart},
  year = {2018},
  month = jun,
  address = {{Toronto, Canada}},
  abstract = {The concept of robust vision is explored as a means to improve autonomous vehicle performance and safety. This research is applicable to both the University of Toronto's self-driving car team, aUToronto, as well as to manufacturers of autonomous road vehicles, who have been criticized for the failures of their vehicles that resulted in injuries and fatalities. The requirements of a robust vision system are identified; chiefly, it must be capable of uncertainty quantification, so this field is introduced and explored with respect to its applications in vision. With this foundation, the most commonly used computer vision algorithms are evaluated for robustness. Some experiments are performed using one of the most robust algorithms identified (Bayesian Neural Networks), on autonomous driving applications to demonstrate the advantages of uncertainty quantification. Noting that a major factor in the lack of usage of robust vision systems in autonomous driving is the computational cost, a proposal is made to use FPGAs to eliminate this relative disadvantage of Bayesian Neural Networks over the current most popular models. If future tests to validate the proposal are successful, this may pave the way for more robust vision systems to be adopted by autonomous vehicle manufacturers.},
  copyright = {All rights reserved},
  school = {University of Toronto},
  type = {B.{{A}}.{{Sc}}. {{Thesis}}}
}

@inproceedings{Jamieson2019,
  title = {The {{Pervasiveness}} of {{Deep Learning}} in {{Robotics Research Does Not Impede Scientific Insights}} into {{Robotics Problems}}},
  booktitle = {Debates on the {{Future}} of {{Robotics Research Workshop}} at {{ICRA}} 2019},
  author = {Jamieson, Stewart},
  year = {2019},
  month = may,
  address = {{Montr\'eal, Canada}},
  copyright = {All rights reserved}
}

@inproceedings{Jamieson2020,
  title = {Active {{Reward Learning}} for {{Co}}-{{Robotic Vision Based Exploration}} in {{Bandwidth Limited Environments}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Jamieson, Stewart and How, Jonathan P. and Girdhar, Yogesh},
  year = {2020},
  month = may,
  pages = {1806--1812},
  publisher = {{IEEE}},
  address = {{Paris, France}},
  issn = {2577-087X},
  doi = {10/gjktt7},
  url = {https://arxiv.org/abs/2003.05016},
  abstract = {We present a novel POMDP problem formulation for a robot that must autonomously decide where to go to collect new and scientifically relevant images given a limited ability to communicate with its human operator. From this formulation we derive constraints and design principles for the observation model, reward model, and communication strategy of such a robot, exploring techniques to deal with the very high-dimensional observation space and scarcity of relevant training data. We introduce a novel active reward learning strategy based on making queries to help the robot minimize path "regret" online, and evaluate it for suitability in autonomous visual exploration through simulations. We demonstrate that, in some bandwidth-limited environments, this novel regret-based criterion enables the robotic explorer to collect up to 17\% more reward per mission than the next-best criterion.},
  copyright = {All rights reserved},
  file = {/home/stewart/sync/Zotero/storage/VG9KXIH8/Jamieson et al. - 2020 - Active Reward Learning for Co-Robotic Vision Based.pdf;/home/stewart/sync/Zotero/storage/B7NFTS8C/citations.html},
  keywords = {Award: Winner of Best Paper Award in Service Robotics,Bandwidth,Computational modeling,Robot sensing systems,Semantics,Trajectory,Video: https://www.youtube.com/watch?v=NH1G8u2hbEU,Visualization}
}

@phdthesis{Jamieson2020a,
  title = {Enabling {{Human}}-{{Robot Cooperation}} in {{Scientific Exploration}} of {{Bandwidth}}-{{Limited Environments}}},
  author = {Jamieson, Stewart},
  year = {2020},
  month = may,
  address = {{Cambridge, MA, USA}},
  doi = {10.1575/1912/25831},
  url = {https://dspace.mit.edu/handle/1721.1/127067},
  abstract = {Contemporary scientific exploration most often takes place in highly remote and dangerous environments, such as in the deep sea and on other planets. These environments are very hostile to humans, which makes robotic exploration the first and often the only option. However, they also impose restrictive limits on how much communication is possible, creating challenges in implementing remote command and control.},
  copyright = {All rights reserved},
  file = {/home/stewart/sync/Zotero/storage/Z5WLMLNK/Jamieson - Enabling Human-Robot Cooperation in Scientific Exp.pdf},
  school = {Massachusetts Institute of Technology},
  type = {Master's {{Thesis}}}
}

@inproceedings{Jamieson2021,
  title = {Multi-{{Robot Distributed Semantic Mapping}} in {{Unfamiliar Environments}} through {{Online Matching}} of {{Learned Representations}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Jamieson, Stewart and Fathian, Kaveh and Khosoussi, Kasra and How, Jonathan P. and Girdhar, Yogesh},
  year = {2021},
  month = may,
  publisher = {{IEEE}},
  address = {{Xi'an, China}},
  url = {http://arxiv.org/abs/2103.14805},
  urldate = {2021-03-30},
  abstract = {We present a solution to multi-robot distributed semantic mapping of novel and unfamiliar environments. Most state-of-the-art semantic mapping systems are based on supervised learning algorithms that cannot classify novel observations online. While unsupervised learning algorithms can invent labels for novel observations, approaches to detect when multiple robots have independently developed their own labels for the same new class are prone to erroneous or inconsistent matches. These issues worsen as the number of robots in the system increases and prevent fusing the local maps produced by each robot into a consistent global map, which is crucial for cooperative planning and joint mission summarization. Our proposed solution overcomes these obstacles by having each robot learn an unsupervised semantic scene model online and use a multiway matching algorithm to identify consistent sets of matches between learned semantic labels belonging to different robots. Compared to the state of the art, the proposed solution produces 20-60\% higher quality global maps that do not degrade even as many more local maps are fused.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {2103.14805},
  eprinttype = {arxiv},
  file = {/home/stewart/sync/Zotero/storage/X8KSBLMW/Jamieson et al. - 2021 - Multi-Robot Distributed Semantic Mapping in Unfami.pdf;/home/stewart/sync/Zotero/storage/7RR3XK8N/2103.html},
  keywords = {â›” No DOI found,Computer Science - Multiagent Systems,Computer Science - Robotics},
  note = {Comment: 7 pages, 6 figures, 1 table; accepted for presentation in IEEE Int. Conf. on Robotics and Automation, ICRA '21, Xi'an, China, June 2021}
}

@inproceedings{Jamieson2021a,
  title = {Communicating {{Efficiently}} to {{Enable Human}}-{{Multi}}-{{Robot Collaboration}} in {{Space Exploration}}},
  booktitle = {Proceedings of {{SpaceCHI}}: {{Human}}-{{Computer Interaction}} for {{Space Exploration}}},
  author = {Jamieson, Stewart and Todd, Jessica E. and How, Jonathan P. and Girdhar, Yogesh},
  year = {2021},
  month = may,
  address = {{Yokohama, Japan}},
  copyright = {All rights reserved}
}


