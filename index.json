[{"authors":["admin"],"categories":null,"content":"I am a PhD student in the MIT-WHOI Joint Program studying Applied Ocean Science and Engineering. I\u0026rsquo;m pursuing the development of a multi-robot system that can autonomously explore and build semantic maps of environments where communication is limited, such as the oceans. My co-supervisors are Dr. Yogesh Girdhar (WHOI) and Dr. Jonathan How (MIT).\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1591563437,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sjamieson.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student in the MIT-WHOI Joint Program studying Applied Ocean Science and Engineering. I\u0026rsquo;m pursuing the development of a multi-robot system that can autonomously explore and build semantic maps of environments where communication is limited, such as the oceans.","tags":null,"title":"Stewart Jamieson","type":"authors"},{"authors":["Stewart Jamieson","Jessica E. Todd","Jonathan P. How","Yogesh Girdhar"],"categories":[],"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617077002,"objectID":"f828700be6cb4c3dfd343ba34783cfe7","permalink":"https://sjamieson.github.io/publication/jamieson-2021-a/","publishdate":"2021-03-30T03:37:56.188139Z","relpermalink":"/publication/jamieson-2021-a/","section":"publication","summary":"","tags":[],"title":"Communicating Efficiently to Enable Human-Multi-Robot Collaboration in Space Exploration","type":"publication"},{"authors":["Stewart Jamieson","Kaveh Fathian","Kasra Khosoussi","Jonathan P. How","Yogesh Girdhar"],"categories":[],"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617077002,"objectID":"01f4ed28895549b4296bac91840be249","permalink":"https://sjamieson.github.io/publication/jamieson-2021/","publishdate":"2021-03-30T03:37:56.109301Z","relpermalink":"/publication/jamieson-2021/","section":"publication","summary":"We present a solution to multi-robot distributed semantic mapping of novel and unfamiliar environments. Most state-of-the-art semantic mapping systems are based on supervised learning algorithms that cannot classify novel observations online. While unsupervised learning algorithms can invent labels for novel observations, approaches to detect when multiple robots have independently developed their own labels for the same new class are prone to erroneous or inconsistent matches. These issues worsen as the number of robots in the system increases and prevent fusing the local maps produced by each robot into a consistent global map, which is crucial for cooperative planning and joint mission summarization. Our proposed solution overcomes these obstacles by having each robot learn an unsupervised semantic scene model online and use a multiway matching algorithm to identify consistent sets of matches between learned semantic labels belonging to different robots. Compared to the state of the art, the proposed solution produces 20-60% higher quality global maps that do not degrade even as many more local maps are fused.","tags":["\"⛔ No DOI found\"","\"Computer Science - Multiagent Systems\"","\"Computer Science - Robotics\""],"title":"Multi-Robot Distributed Semantic Mapping in Unfamiliar Environments through Online Matching of Learned Representations","type":"publication"},{"authors":["Stewart Jamieson","Jonathan P. How","Yogesh Girdhar"],"categories":null,"content":"","date":1590997500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591563354,"objectID":"ed598ad4366e15de1d7c62e6173dc029","permalink":"https://sjamieson.github.io/talk/2020-icra-presentation/","publishdate":"2020-05-31T12:00:00-04:00","relpermalink":"/talk/2020-icra-presentation/","section":"talk","summary":"We present a novel POMDP problem formulation for  a  robot  that  must  autonomously  decide  where  to  go  tocollect  new  and  scientifically  relevant  images  given  a  limited ability  to  communicate  with  its  human  operator.  From  this formulation we derive constraints and design principles for the observation model, reward model, and communication strategy of such a robot, exploring techniques to deal with the very high-dimensional observation space and scarcity of relevant training data.  We  introduce  a  novel  active  reward  learning  strategy based  on  making  queries  to  help  the  robot  minimize  path “regret”  online,  and  evaluate  it  for  suitability  in  autonomous visual exploration through simulations. We demonstrate that, in some  bandwidth-limited  environments,  this  novel  regret-based criterion enables the robotic explorer to collect up to 17% more reward  per  mission  than  the  next-best criterion.","tags":[],"title":"ICRA 2020 Presentation: Active Reward Learning for Co-Robotic Vision Based Exploration","type":"talk"},{"authors":["Stewart Jamieson","Jonathan P. How","Yogesh Girdhar"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617077002,"objectID":"e2fd059aaf0310482b8a4d9988609e95","permalink":"https://sjamieson.github.io/publication/jamieson-2020/","publishdate":"2021-03-30T03:37:55.946375Z","relpermalink":"/publication/jamieson-2020/","section":"publication","summary":"We present a novel POMDP problem formulation for a robot that must autonomously decide where to go to collect new and scientifically relevant images given a limited ability to communicate with its human operator. From this formulation we derive constraints and design principles for the observation model, reward model, and communication strategy of such a robot, exploring techniques to deal with the very high-dimensional observation space and scarcity of relevant training data. We introduce a novel active reward learning strategy based on making queries to help the robot minimize path \\\"regret\\\" online, and evaluate it for suitability in autonomous visual exploration through simulations. We demonstrate that, in some bandwidth-limited environments, this novel regret-based criterion enables the robotic explorer to collect up to 17% more reward per mission than the next-best criterion.","tags":["\"Award: Winner of Best Paper Award in Service Robotics\"","\"Bandwidth\"","\"Computational modeling\"","\"Robot sensing systems\"","\"Semantics\"","\"Trajectory\"","\"Video: https://www.youtube.com/watch?v=NH1G8u2hbEU\"","\"Visualization\""],"title":"Active Reward Learning for Co-Robotic Vision Based Exploration in Bandwidth Limited Environments","type":"publication"},{"authors":["Stewart Jamieson"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617077002,"objectID":"358c87182de7439b25dca65fae588728","permalink":"https://sjamieson.github.io/publication/jamieson-2020-a/","publishdate":"2021-03-30T03:37:56.032449Z","relpermalink":"/publication/jamieson-2020-a/","section":"publication","summary":"Contemporary scientific exploration most often takes place in highly remote and dangerous environments, such as in the deep sea and on other planets. These environments are very hostile to humans, which makes robotic exploration the first and often the only option. However, they also impose restrictive limits on how much communication is possible, creating challenges in implementing remote command and control.","tags":null,"title":"Enabling Human-Robot Cooperation in Scientific Exploration of Bandwidth-Limited Environments","type":"publication"},{"authors":["Stewart Jamieson"],"categories":null,"content":"","date":1558710900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591563354,"objectID":"4cbe7739dbd2f071afee7fd88c76fe2a","permalink":"https://sjamieson.github.io/talk/2019-icra-debate/","publishdate":"2019-05-18T17:36:43-04:00","relpermalink":"/talk/2019-icra-debate/","section":"talk","summary":"Deep learning will play an invaluable role in advancing nearly every field of robotics research.","tags":[],"title":"The Importance of Deep Learning in Robotics Research","type":"talk"},{"authors":null,"categories":null,"content":"","date":1558214359,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558217581,"objectID":"752c584373794e652b8b17c3c27e33b6","permalink":"https://sjamieson.github.io/project/co-robotic-exploration/","publishdate":"2019-05-18T17:19:19-04:00","relpermalink":"/project/co-robotic-exploration/","section":"project","summary":"Employing teams of robots and scientists to work together to better understand Earth's oceans.","tags":[],"title":"Co-Robotic Scientific Exploration","type":"project"},{"authors":["Yogesh Girdhar","Levi Cai","Stewart Jamieson","Nathan McGuire","Genevieve Flaspohler","Stefano Suman","Brian Claus"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617077002,"objectID":"6ff88e8403788dffb3b8bbe73df2c459","permalink":"https://sjamieson.github.io/publication/girdhar-2019-icra/","publishdate":"2021-03-30T03:37:55.713849Z","relpermalink":"/publication/girdhar-2019-icra/","section":"publication","summary":"This paper proposes a bandwidth tunable technique for real-time probabilistic scene modeling and mapping to enable co-robotic exploration in communication constrained environments such as the deep sea. The parameters of the system enable the user to characterize the scene complexity represented by the map, which in turn determines the bandwidth requirements. The approach is demonstrated using an underwater robot that learns an unsupervised scene model of the environment and then uses this scene model to communicate the spatial distribution of various high-level semantic scene constructs to a human operator. Preliminary experiments in an artificially constructed tank environment as well as simulated missions over a 10m× 10m coral reef using real data show the tunability of the maps to different bandwidth constraints and science interests. To our knowledge this is the first paper to quantity how the free parameters of the unsupervised scene model impact both the scientific utility of and bandwidth required to communicate the resulting scene model.","tags":["\"Bandwidth\"","\"Bayes methods\"","\"Data models\"","\"Oceans\"","\"Robot sensing systems\"","\"Visualization\""],"title":"Streaming Scene Maps for Co-Robotic Exploration in Bandwidth Limited Environments","type":"publication"},{"authors":["Stewart Jamieson"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617077002,"objectID":"258c3b45bc287c1657e8e7ec6bcc9a6d","permalink":"https://sjamieson.github.io/publication/jamieson-2019/","publishdate":"2021-03-30T03:37:55.87438Z","relpermalink":"/publication/jamieson-2019/","section":"publication","summary":"","tags":null,"title":"The Pervasiveness of Deep Learning in Robotics Research Does Not Impede Scientific Insights into Robotics Problems","type":"publication"},{"authors":["Juan Jose Garau Luis","Stewart Jamieson","Sarah Keren","Rajat Talak"],"categories":null,"content":"","date":1554906600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591563354,"objectID":"b201ba087bd08f343f440d2ab2d70024","permalink":"https://sjamieson.github.io/talk/2019-bayesian-adaptive-sampling/","publishdate":"2019-05-18T17:37:14-04:00","relpermalink":"/talk/2019-bayesian-adaptive-sampling/","section":"talk","summary":"","tags":[],"title":"Multi-Robot Adaptive Sampling","type":"talk"},{"authors":["Stewart Jamieson"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617077002,"objectID":"e228c4293597f6c22850983e2184d04c","permalink":"https://sjamieson.github.io/publication/jamieson-2018/","publishdate":"2021-03-30T03:37:55.795673Z","relpermalink":"/publication/jamieson-2018/","section":"publication","summary":"The concept of robust vision is explored as a means to improve autonomous vehicle performance and safety. This research is applicable to both the University of Toronto's self-driving car team, aUToronto, as well as to manufacturers of autonomous road vehicles, who have been criticized for the failures of their vehicles that resulted in injuries and fatalities. The requirements of a robust vision system are identified; chiefly, it must be capable of uncertainty quantification, so this field is introduced and explored with respect to its applications in vision. With this foundation, the most commonly used computer vision algorithms are evaluated for robustness. Some experiments are performed using one of the most robust algorithms identified (Bayesian Neural Networks), on autonomous driving applications to demonstrate the advantages of uncertainty quantification. Noting that a major factor in the lack of usage of robust vision systems in autonomous driving is the computational cost, a proposal is made to use FPGAs to eliminate this relative disadvantage of Bayesian Neural Networks over the current most popular models. If future tests to validate the proposal are successful, this may pave the way for more robust vision systems to be adopted by autonomous vehicle manufacturers.","tags":null,"title":"Deep Learning for Robust Vision in Realtime Autonomous Driving","type":"publication"}]